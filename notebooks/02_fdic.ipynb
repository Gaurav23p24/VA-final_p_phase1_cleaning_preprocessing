{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 FDIC Failed Bank List — Cleaning & Export\n",
    "\n",
    "Goal: Clean FDIC failed banks data with robust encoding, standardize states, create Power BI–ready keys (`bank_key`, `geo_key`, `date_key`), export `fact_fdic_failures.csv` and `dim_bank.csv`.\n",
    "\n",
    "Guardrails: work on copies, no inplace, explicit conversions, validate early, conservative cleaning, document decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T09:23:43.068438Z",
     "iopub.status.busy": "2025-11-16T09:23:43.068438Z",
     "iopub.status.idle": "2025-11-16T09:23:43.073341Z",
     "shell.execute_reply": "2025-11-16T09:23:43.073341Z"
    }
   },
   "outputs": [],
   "source": [
    "# Helpers: drop unnamed cols and sanitize names\n",
    "import re\n",
    "\n",
    "def drop_unnamed(df):\n",
    "    return df.loc[:, ~df.columns.astype(str).str.match(r'^Unnamed')]\n",
    "\n",
    "def sanitize_columns(df):\n",
    "    def _san(c):\n",
    "        c = str(c).strip().lower()\n",
    "        c = re.sub(r\"\\s+\", \"_\", c)\n",
    "        c = re.sub(r\"[^a-z0-9_]+\", \"_\", c)\n",
    "        c = re.sub(r\"_+\", \"_\", c).strip('_')\n",
    "        return c\n",
    "    df = df.copy()\n",
    "    df.columns = [_san(c) for c in df.columns]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What: Import libraries and constants.\n",
    "Why: Standardize environment and explicit config.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T09:23:43.074347Z",
     "iopub.status.busy": "2025-11-16T09:23:43.074347Z",
     "iopub.status.idle": "2025-11-16T09:23:43.347466Z",
     "shell.execute_reply": "2025-11-16T09:23:43.347466Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 160)\n",
    "\n",
    "ANALYSIS_START_YEAR = 2000\n",
    "ANALYSIS_END_YEAR = 2024\n",
    "ENCODINGS_TRY = [\"cp1252\", \"latin-1\"]\n",
    "US_STATE_CODE_LENGTH = 2\n",
    "USA_GEO_KEY = \"USA\"\n",
    "\n",
    "ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\")) if os.path.basename(os.getcwd()) == \"notebooks\" else os.getcwd()\n",
    "RAW = os.path.join(ROOT, \"original_data\")\n",
    "CLEAN = os.path.join(ROOT, \"data\", \"cleaned\")\n",
    "os.makedirs(CLEAN, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What: Load CSV with encoding fallbacks into a copy.\n",
    "Why: Handle known encoding issues without mutating raw data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T09:23:43.350124Z",
     "iopub.status.busy": "2025-11-16T09:23:43.348866Z",
     "iopub.status.idle": "2025-11-16T09:23:43.362116Z",
     "shell.execute_reply": "2025-11-16T09:23:43.362116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded with encoding: cp1252\n",
      "Shape: (572, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bank Name</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Cert</th>\n",
       "      <th>Acquiring Institution</th>\n",
       "      <th>Closing Date</th>\n",
       "      <th>Fund</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Santa Anna National Bank</td>\n",
       "      <td>Santa Anna</td>\n",
       "      <td>TX</td>\n",
       "      <td>5520</td>\n",
       "      <td>Coleman County State Bank</td>\n",
       "      <td>27-Jun-25</td>\n",
       "      <td>10549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pulaski Savings Bank</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>28611</td>\n",
       "      <td>Millennium Bank</td>\n",
       "      <td>17-Jan-25</td>\n",
       "      <td>10548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>First National Bank of Lindsay</td>\n",
       "      <td>Lindsay</td>\n",
       "      <td>OK</td>\n",
       "      <td>4134</td>\n",
       "      <td>First Bank &amp; Trust Co.</td>\n",
       "      <td>18-Oct-24</td>\n",
       "      <td>10547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Republic First Bank dba Republic Bank</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>PA</td>\n",
       "      <td>27332</td>\n",
       "      <td>Fulton Bank, National Association</td>\n",
       "      <td>26-Apr-24</td>\n",
       "      <td>10546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Citizens Bank</td>\n",
       "      <td>Sac City</td>\n",
       "      <td>IA</td>\n",
       "      <td>8758</td>\n",
       "      <td>Iowa Trust &amp; Savings Bank</td>\n",
       "      <td>3-Nov-23</td>\n",
       "      <td>10545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Bank Name          City  State   Cert              Acquiring Institution  Closing Date    Fund\n",
       "0           The Santa Anna National Bank    Santa Anna     TX   5520          Coleman County State Bank     27-Jun-25  10549\n",
       "1                   Pulaski Savings Bank       Chicago     IL  28611                    Millennium Bank     17-Jan-25  10548\n",
       "2         First National Bank of Lindsay       Lindsay     OK   4134             First Bank & Trust Co.     18-Oct-24  10547\n",
       "3  Republic First Bank dba Republic Bank  Philadelphia     PA  27332  Fulton Bank, National Association     26-Apr-24  10546\n",
       "4                          Citizens Bank      Sac City     IA   8758          Iowa Trust & Savings Bank      3-Nov-23  10545"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Robust load with encoding fallbacks (work on copy)\n",
    "raw_path = os.path.join(RAW, \"FDIC Failed Bank List (US).csv\")\n",
    "df_raw = None\n",
    "for enc in ENCODINGS_TRY:\n",
    "    try:\n",
    "        df_raw = pd.read_csv(raw_path, encoding=enc)\n",
    "        print(f\"Loaded with encoding: {enc}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Failed with {enc}: {e}\")\n",
    "\n",
    "if df_raw is None:\n",
    "    raise RuntimeError(\"Unable to load FDIC CSV with provided encodings.\")\n",
    "\n",
    "df = df_raw.copy()\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What: Inspect schema; detect state/city/bank/closing-date columns.\n",
    "Why: Identify dimensions and key fields for modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T09:23:43.382807Z",
     "iopub.status.busy": "2025-11-16T09:23:43.382807Z",
     "iopub.status.idle": "2025-11-16T09:23:43.417053Z",
     "shell.execute_reply": "2025-11-16T09:23:43.417053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 572 entries, 0 to 571\n",
      "Data columns (total 7 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   Bank Name               572 non-null    object\n",
      " 1   City                    572 non-null    object\n",
      " 2   State                   572 non-null    object\n",
      " 3   Cert                    572 non-null    int64 \n",
      " 4   Acquiring Institution   572 non-null    object\n",
      " 5   Closing Date            572 non-null    object\n",
      " 6   Fund                    572 non-null    int64 \n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 31.4+ KB\n",
      "None\n",
      "                        count unique               top freq          mean           std     min       25%      50%       75%      max\n",
      "Bank Name                 572    553  First State Bank    3           NaN           NaN     NaN       NaN      NaN       NaN      NaN\n",
      "City                      572    438           Chicago   21           NaN           NaN     NaN       NaN      NaN       NaN      NaN\n",
      "State                     572     44                GA   93           NaN           NaN     NaN       NaN      NaN       NaN      NaN\n",
      "Cert                    572.0    NaN               NaN  NaN  31553.940559  16498.371919    91.0  20025.75  32016.5   35360.0  59017.0\n",
      "Acquiring Institution     572    306       No Acquirer   31           NaN           NaN     NaN       NaN      NaN       NaN      NaN\n",
      "Closing Date              572    267         30-Oct-09    9           NaN           NaN     NaN       NaN      NaN       NaN      NaN\n",
      "Fund                    572.0    NaN               NaN  NaN  10044.863636   1108.318974  4645.0  10118.75  10261.5  10404.25  10549.0\n",
      "Columns detected: State  City  Bank Name  Closing Date \n",
      "Invalid closing dates: 0\n",
      "Rows within 2000-2024: 570 / 572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gaura\\AppData\\Local\\Temp\\ipykernel_12800\\3339774408.py:20: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  d_close = pd.to_datetime(df_clean[col_close], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "print(df.info())\n",
    "print(df.describe(include='all').T.head(20))\n",
    "\n",
    "# Canonical column name guesses (adjust as needed)\n",
    "col_state = next((c for c in df.columns if c.strip().lower() in (\"st\", \"state\")), None)\n",
    "col_city = next((c for c in df.columns if c.strip().lower() == \"city\"), None)\n",
    "col_bank = next((c for c in df.columns if c.strip().lower() in (\"bank name\", \"bank_name\", \"bank\")), None)\n",
    "col_close = next((c for c in df.columns if \"closing\" in c.strip().lower() and \"date\" in c.strip().lower()), None)\n",
    "\n",
    "print(\"Columns detected:\", col_state, col_city, col_bank, col_close)\n",
    "assert col_state and col_city and col_bank and col_close, \"Required columns not detected; please set manually.\"\n",
    "\n",
    "# Clean basics\n",
    "df_clean = df.copy()\n",
    "df_clean[col_state] = df_clean[col_state].astype(str).str.strip().str.upper()\n",
    "df_clean[col_city] = df_clean[col_city].astype(str).str.strip()\n",
    "df_clean[col_bank] = df_clean[col_bank].astype(str).str.strip()\n",
    "\n",
    "# Parse closing date\n",
    "d_close = pd.to_datetime(df_clean[col_close], errors='coerce')\n",
    "invalid = d_close.isna().sum()\n",
    "print(\"Invalid closing dates:\", invalid)\n",
    "\n",
    "# Temporal scope filter\n",
    "mask_year = (d_close.dt.year >= ANALYSIS_START_YEAR) & (d_close.dt.year <= ANALYSIS_END_YEAR)\n",
    "print(f\"Rows within {ANALYSIS_START_YEAR}-{ANALYSIS_END_YEAR}:\", mask_year.sum(), \"/\", len(df_clean))\n",
    "df_clean = df_clean.loc[mask_year].copy()\n",
    "\n",
    "# Keys\n",
    "# geo_key: USA-{ST}\n",
    "df_clean['geo_key'] = 'USA-' + df_clean[col_state]\n",
    "# date_key: YYYYMMDD\n",
    "s_date_key = d_close.loc[mask_year].dt.strftime('%Y%m%d').astype(int)\n",
    "df_clean['date_key'] = s_date_key\n",
    "\n",
    "# bank_key: stable hash of bank_name_clean|city|state_code\n",
    "def stable_bank_key(name, city, st):\n",
    "    s = f\"{name}|{city}|{st}\".encode('utf-8')\n",
    "    return int(hashlib.sha1(s).hexdigest()[:12], 16)\n",
    "\n",
    "df_clean['bank_key'] = [stable_bank_key(n, c, s) for n, c, s in zip(df_clean[col_bank], df_clean[col_city], df_clean[col_state])]\n",
    "\n",
    "# dim_bank\n",
    "dim_bank = df_clean[[\"bank_key\", col_bank, col_city, col_state]].drop_duplicates().rename(columns={col_bank: \"bank_name_clean\", col_city: \"city\", col_state: \"state_code\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What: Standardize values; filter by year; create keys and `dim_bank`.\n",
    "Why: Enable joins in Power BI and maintain consistent grain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T09:23:43.419277Z",
     "iopub.status.busy": "2025-11-16T09:23:43.418277Z",
     "iopub.status.idle": "2025-11-16T09:23:43.425697Z",
     "shell.execute_reply": "2025-11-16T09:23:43.425697Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dim_geography with state-level rows: G:\\ACADEMIA\\VA 5122\\Final Project\\phase1_cleaning_preprocessing\\data\\cleaned\\dim_geography.csv\n"
     ]
    }
   ],
   "source": [
    "# Augment dim_geography with USA-{ST} entries\n",
    "try:\n",
    "    dim_geo_path = os.path.join(CLEAN, 'dim_geography.csv')\n",
    "    # Build state rows from df_clean\n",
    "    states = df_clean['geo_key'].dropna().drop_duplicates()\n",
    "    state_rows = pd.DataFrame({\n",
    "        'geo_key': states,\n",
    "        'country_iso3': 'USA',\n",
    "        'country_name': 'United States',\n",
    "        'state_code': states.str.replace('USA-','', regex=False),\n",
    "        'is_usa': 1\n",
    "    })\n",
    "    if os.path.exists(dim_geo_path):\n",
    "        existing = pd.read_csv(dim_geo_path)\n",
    "        existing = sanitize_columns(existing)\n",
    "        combined = pd.concat([existing, state_rows], ignore_index=True).drop_duplicates(subset=['geo_key'])\n",
    "    else:\n",
    "        combined = state_rows\n",
    "    combined = sanitize_columns(combined)\n",
    "    combined.to_csv(dim_geo_path, index=False, encoding='utf-8')\n",
    "    print('Updated dim_geography with state-level rows:', dim_geo_path)\n",
    "except Exception as e:\n",
    "    print('dim_geography update failed:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T09:23:43.425697Z",
     "iopub.status.busy": "2025-11-16T09:23:43.425697Z",
     "iopub.status.idle": "2025-11-16T09:23:43.433663Z",
     "shell.execute_reply": "2025-11-16T09:23:43.433663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: G:\\ACADEMIA\\VA 5122\\Final Project\\phase1_cleaning_preprocessing\\data\\cleaned\\fdic_failed_banks_cleaned.csv\n",
      "Wrote: G:\\ACADEMIA\\VA 5122\\Final Project\\phase1_cleaning_preprocessing\\data\\cleaned\\dim_bank.csv\n"
     ]
    }
   ],
   "source": [
    "# Exports (Power BI friendly)\n",
    "fact_fdic = df_clean.copy()\n",
    "fact_fdic.columns = [str(c).strip().lower().replace(' ', '_') for c in fact_fdic.columns]\n",
    "dim_bank.columns = [str(c).strip().lower().replace(' ', '_') for c in dim_bank.columns]\n",
    "\n",
    "fact_path = os.path.join(CLEAN, 'fdic_failed_banks_cleaned.csv')\n",
    "bank_path = os.path.join(CLEAN, 'dim_bank.csv')\n",
    "\n",
    "fact_fdic.to_csv(fact_path, index=False, encoding='utf-8')\n",
    "dim_bank.to_csv(bank_path, index=False, encoding='utf-8')\n",
    "\n",
    "print('Wrote:', fact_path)\n",
    "print('Wrote:', bank_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What: Export fact and dimension CSVs.\n",
    "Why: Deliver Power BI–ready files with consistent naming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T09:23:43.434668Z",
     "iopub.status.busy": "2025-11-16T09:23:43.434668Z",
     "iopub.status.idle": "2025-11-16T09:23:43.438734Z",
     "shell.execute_reply": "2025-11-16T09:23:43.438734Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fact_fdic shape: (570, 10)\n",
      "dim_bank shape: (570, 4)\n",
      "Unique bank_key in dim_bank: True\n",
      "Orphan bank_key in fact_fdic: 0\n",
      "State code length ok: True\n",
      "Min/Max date_key: 20001013 20241018\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "print('fact_fdic shape:', fact_fdic.shape)\n",
    "print('dim_bank shape:', dim_bank.shape)\n",
    "\n",
    "# Key uniqueness and FK coverage\n",
    "print('Unique bank_key in dim_bank:', dim_bank['bank_key'].is_unique)\n",
    "print('Orphan bank_key in fact_fdic:', (~fact_fdic['bank_key'].isin(dim_bank['bank_key'])).sum())\n",
    "print('State code length ok:', (dim_bank['state_code'].astype(str).str.len() == 2).all())\n",
    "\n",
    "# Date range sanity\n",
    "print('Min/Max date_key:', fact_fdic['date_key'].min(), fact_fdic['date_key'].max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What: Validate shapes, key uniqueness, FK coverage, and date ranges.\n",
    "Why: Prevent orphaned records and ensure relational integrity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decisions & Notes\n",
    "\n",
    "- Encoding: tried cp1252 then latin-1; document any bad characters observed.\n",
    "- State codes standardized to USPS 2 letters; anomalies should be listed here.\n",
    "- Keys: `bank_key` (stable SHA1-based), `geo_key` (USA-{ST}), `date_key` (YYYYMMDD).\n",
    "- Any duplicate handling or name canonicalization decisions must be documented below.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
